{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Biodiversity report generator\n",
    "---\n",
    "This notebook shows the inner workings of the report generator. It combines the \"greeneryDataForNeighborhoodV2\" and \"retrieveGreeneryGoals\" to create an overview of a greenery goal and the progress that has been made to achieve this goal.\n"
   ],
   "id": "e9eb7d5fa39038fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 0: Imports\n",
   "id": "308e71c660a7ab23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T18:44:09.125126Z",
     "start_time": "2025-05-19T18:44:09.078405Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.sequential import SequentialChain\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains import TransformChain\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.memory import SimpleMemory\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "from langchain_community.docstore import InMemoryDocstore\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import StaticPool\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from uuid import uuid4\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import sqlite3\n",
    "import faiss"
   ],
   "id": "48fe2553ab822baf",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Choose a large language model and embedding model\n",
    "First the LLM and embedding model has to be chosen. There are many options, but a few have been tested and can easily be changed by runing it's specific codeblock."
   ],
   "id": "d2d8d2e222fb6696"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LLM\n",
    "Choose one of the implemented LLMs: Llama3, o3-mini or 4o-mini.\n"
   ],
   "id": "b05e975155a0d7e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option A: Llama3 (via Ollama, running locally)",
   "id": "9b72011bcc495303"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:11:26.475236Z",
     "start_time": "2025-05-13T13:11:26.428027Z"
    }
   },
   "cell_type": "code",
   "source": "chosen_llm = ChatOllama(base_url='http://localhost:11434', model=\"llama3\")",
   "id": "a1a4046e1a641a9b",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option B: o3-mini (via Microsoft Azure, running in cloud):",
   "id": "f77781c99b48898a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:11:26.570262Z",
     "start_time": "2025-05-13T13:11:26.506718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "chosen_llm = AzureChatOpenAI(model =\"o3-mini\", api_version=\"2025-01-01-preview\", azure_endpoint=\"https://56948-m9bdjgpg-eastus2.cognitiveservices.azure.com/openai/deployments/o3-mini/chat/completions?api-version=2025-01-01-preview\", api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"))"
   ],
   "id": "dae3e6325e0ba1a2",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option C: gpt-4o-mini (via Microsoft Azure, running in cloud):",
   "id": "5a9dce9213b110dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T18:49:05.021452Z",
     "start_time": "2025-05-19T18:49:04.897833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "chosen_llm = AzureChatOpenAI(model=\"gpt-4o-mini\", api_version=\"2025-01-01-preview\",\n",
    "                             azure_endpoint=\"https://56948-m9bdjgpg-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview\",\n",
    "                             api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"))"
   ],
   "id": "260f70cc78667853",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option D: Gemini 2.0 Flash (via Google API, running in cloud)",
   "id": "dc586b93705b7b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:02:16.231289Z",
     "start_time": "2025-05-19T19:02:16.195655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "chosen_llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\",\n",
    "                                    api_key=os.environ.get(\"GOOGLE_API_KEY\"))"
   ],
   "id": "4a8226b52d79761",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option E: Gemini 2.5 Flash (via Google API, running in cloud)",
   "id": "a360451a477dcb76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:07:21.861096Z",
     "start_time": "2025-05-19T19:07:21.817199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "chosen_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-04-17\",\n",
    "                                    api_key=os.environ.get(\"GOOGLE_API_KEY\"))"
   ],
   "id": "87461f5cd0cb0c2a",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embedding model\n",
    "Choose one of the implemented models: all-MiniLM-L6-v2 or all-mpnet-base-v2"
   ],
   "id": "c33ab39567730e0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option A: all-MiniLM-L6-v2 (faster, but worse)",
   "id": "87baa1708087f6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T13:11:28.224624Z",
     "start_time": "2025-05-13T13:11:26.705111Z"
    }
   },
   "cell_type": "code",
   "source": "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")",
   "id": "70c92f2cf6c4481f",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option B: all-mpnet-base-v2 (slower, but better)",
   "id": "4175768dc151a01e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T18:44:44.598410Z",
     "start_time": "2025-05-19T18:44:39.471184Z"
    }
   },
   "cell_type": "code",
   "source": "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")",
   "id": "4db0e52535159ea5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 2: Create the chains\n",
    "The second step is to create the chains that will retrieve the information that is needed to generate biodiversity reports. This information consists of a greenery goal and a greenery percentage of a specific municipality."
   ],
   "id": "55d98a768d01d87b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chain A: Retrieve greenery goal\n",
    "The first chain is able to retrieve a greenery goal. This goal will be retrieved with a Q&A chain that uses RAG to analyse multiple document chunks. The data that the chain needs will first be prepared to help the LLM analyse the information better and after that the chain will be created that will (later) analyse the prepared data."
   ],
   "id": "fd68ef7eaca25039"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data preparation: save document chunks in a vector database\n",
    "First a pdf that contains a greenery goal will be loaded. The text of this document will be extracted and will then be chunked into smaller pieces. After that a vector database will be created and the chunks will be saved into it."
   ],
   "id": "e3931f1463f62da3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T18:45:29.166275Z",
     "start_time": "2025-05-19T18:45:00.132886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load file\n",
    "file_path = \"./GroenvisieSchiedam.pdf\"\n",
    "doc_reader = PdfReader(file_path)\n",
    "\n",
    "# Extract page content\n",
    "raw_text = ''\n",
    "for page in doc_reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "\n",
    "print(\"Amount of characters raw text: \" + str(len(raw_text)))\n",
    "\n",
    "# Split the page content into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    ")\n",
    "chunks = text_splitter.split_text(raw_text)\n",
    "\n",
    "print(\"Amount of characters first chunk: \" + str(len(chunks[0])))\n",
    "\n",
    "# Create vector store\n",
    "index = faiss.IndexFlatL2(len(embedding_model.embed_query(chunks[0]))) # Calculates the amount of dimensions the chunk's vector has\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embedding_model,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# Give chunks UUIDS\n",
    "uuids = []\n",
    "for chunk in range(len(chunks)):\n",
    "    uuids.append(str(uuid4()))\n",
    "\n",
    "# Create embeddings of chunks and store them in the vector database\n",
    "vector_store.add_texts(texts=chunks, ids=uuids)\n",
    "\n",
    "print(\"\\n UUIDs of items stored in vector database: \" + str(uuids))"
   ],
   "id": "19a6ce21b1b45da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of characters raw text: 9039\n",
      "Amount of characters first chunk: 627\n",
      "\n",
      " UUIDs of items stored in vector database: ['6d1068fd-9b44-4166-a5f8-345e59eff50e', '2485b8b3-e46e-4281-9ce4-faf5d9e07cd0', '87c9bbfc-92f8-41a6-84c4-d8d596a2ebdb', 'f1d2ec33-adc8-43fd-9240-cfa982f99de2', '1a10da17-4123-4cfb-99be-e43d5930ce34', '7cf09a24-7e66-4033-81aa-3abece8acdfc', '2ab1bdd7-bddf-4000-b942-e2d76fc45f2b', 'd3ab8d35-583e-4675-b8c6-27f64de8139f', 'c606de4a-3810-4012-9a89-016e66a53b0f', 'b1aa68dc-34c9-4448-a9a7-f42ee3cdf853', '4c2bd681-fc2d-41bd-8201-069f00ff546f', '03fc2f62-de9b-4769-9e82-4014984b8854']\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Q&A chain\n",
    "Create a QA chain that is able to do a similarity search for the 10 most relevant chunks and analyze these chunks to find the percentage asked."
   ],
   "id": "7b12f0edf62ef178"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:07:27.544163Z",
     "start_time": "2025-05-19T19:07:27.510439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create prompt template\n",
    "template = \"\"\"\n",
    "Je bent een documentenanalist. Gebruik de aangeleverde documenten om het percentage te vinden dat de vraag beantwoord. Geef als antwoord alleen het percentage, verder niks.\n",
    "\n",
    "Vraag: {question}\n",
    "Aangeleverde documenten: {context}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
    "\n",
    "# Create Q&A chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chosen_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), # This will do the similarity search for the 10 most relevant chunks and adds them to the {context} variable\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template},\n",
    "    output_key=\"greeneryGoal\"\n",
    ")\n",
    "\n",
    "print(\"Prompt die is meegegeven aan chain: \", prompt_template.format(question=\"[hier komt de ingevulde vraag te staan]\", context=\"[hier komen de aangeleverde document chunks te staan]\"))"
   ],
   "id": "46f86546e27d844c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt die is meegegeven aan chain:  \n",
      "Je bent een documentenanalist. Gebruik de aangeleverde documenten om het percentage te vinden dat de vraag beantwoord. Geef als antwoord alleen het percentage, verder niks.\n",
      "\n",
      "Vraag: [hier komt de ingevulde vraag te staan]\n",
      "Aangeleverde documenten: [hier komen de aangeleverde document chunks te staan]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chain B: Retrieve greenery percentage\n",
    "The second chain is able retreive the greenery percentage of a specific municipality. This is done with a SQL-agent chain. The data that the agent needs will first be prepared to help the agent analyse the information better and after that the chain will be created that will (later) analyse the prepared data."
   ],
   "id": "5427c2a36e9cedf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data preparation: store greenery percentages in SQlite database\n",
    "First the database will be created by building a SQlite database and creating a \"database engine\" object that will be used for creating the agent's toolkit. As soon as that is done the dataset will be retrieved via an API-request and will then be stored in the database."
   ],
   "id": "742a42304e704597"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T18:45:43.311660Z",
     "start_time": "2025-05-19T18:45:43.095294Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create database\n",
    "con = sqlite3.connect(\"greeneryPercentages.db\", check_same_thread=False)\n",
    "cur = con.cursor()\n",
    "\n",
    "# Create municipalities table in database\n",
    "try:\n",
    "    cur.execute(\"CREATE TABLE municipalities(name varchar(255), greeneryPercentage float)\")\n",
    "    print(\"Table created successfully.\")\n",
    "\n",
    "except:\n",
    "    cur.execute(\"DROP TABLE municipalities\")\n",
    "    cur.execute(\"CREATE TABLE municipalities(name varchar(255), greeneryPercentage float)\")\n",
    "    print(\"Table has been reset, because it already existed.\")\n",
    "\n",
    "# Create database engine object\n",
    "engine = create_engine(\n",
    "        \"sqlite://\",\n",
    "        creator=lambda: con,\n",
    "        poolclass=StaticPool,\n",
    "        connect_args={\"check_same_thread\": False},\n",
    "    )\n",
    "db = SQLDatabase(engine)\n",
    "\n",
    "# Retrieve dataset\n",
    "dataset = requests.get('https://data.rivm.nl/geo/ank/ows?service=WFS&request=GetFeature&typeName=rivm_2022_groenpercentage_kaart_per_gemeente&propertyName=gm_naam,_mean&outputFormat=json').json()\n",
    "\n",
    "# Extract properties and put them into a list\n",
    "properties = []\n",
    "for feature in dataset[\"features\"]:\n",
    "    property = feature[\"properties\"]\n",
    "    properties.append((property[\"gm_naam\"], property[\"_mean\"]))\n",
    "\n",
    "# Save the items stored in the list into the database\n",
    "i=0\n",
    "while i < len(properties):\n",
    "    cur.execute(\"INSERT INTO municipalities VALUES (?,?)\", properties[i])\n",
    "    i = i + 1\n",
    "con.commit()"
   ],
   "id": "9b073020630bf673",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has been reset, because it already existed.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create SQL-agent chain\n",
    "Create a SQL-agent chain that is able to analyse the created database and create a SQL-query for it to answer the question when it is executed."
   ],
   "id": "f4fc9eec4fd9d2bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:07:32.368361Z",
     "start_time": "2025-05-19T19:07:32.332780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create toolkit:\n",
    "toolkit = SQLDatabaseToolkit(db=db,llm=chosen_llm)\n",
    "\n",
    "# Create agent chain\n",
    "sql_agent_chain = create_sql_agent(\n",
    "    llm=chosen_llm,\n",
    "    toolkit=toolkit,\n",
    "    verbose=False,\n",
    "    output_key=\"greeneryPercentage\",\n",
    "    handleParsingErrors=True\n",
    ")\n",
    "\n",
    "# Transform function that returns the result of the agent in a dictionary (making it compatible with the other chains)\n",
    "def run_sql_agent(inputs):\n",
    "    result = sql_agent_chain.run(inputs[\"greeneryPercentageInputPrompt\"])\n",
    "    return {\"greeneryPercentage\": result}\n",
    "\n",
    "# Transform Chain that calls the transform function\n",
    "greeneryPercentageChain = TransformChain(\n",
    "    input_variables=[\"greeneryPercentageInputPrompt\"],\n",
    "    output_variables=[\"greeneryPercentage\"],\n",
    "    transform=run_sql_agent\n",
    ")"
   ],
   "id": "15fa21b9639e94f4",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chain C: Combine greenery goal and greenery percentage\n",
    "The third chain will analyse a greenery goal and percentage and write a few lines about the progress towards the goal.\n"
   ],
   "id": "a56af0b925d1e5bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:07:35.440025Z",
     "start_time": "2025-05-19T19:07:35.408074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create prompt template\n",
    "template = \"\"\"\n",
    "Je bent een biodiversiteit analyst. Je krijgt een groenpercentage van een gemeente en de doelstelling van deze gemeente, aan jou de taak om deze informatie te tonen aan de gebruiker en te vertellen hoever de doelstelling behaald is. Jouw analyse komt als tekstje op een infographic te staan, beschrijf daarom de resultaten in maximaal 150 woorden. Zorg ervoor dat je een formele schrijfstijl gebruikt. Het format hiervoor is als volgt:\n",
    "'Gemeente: <de gemeente waar je het over hebt>\n",
    "Doelstelling: <het groenpercentage>\n",
    "Huidige hoeveelheid groen: <de groendoelstelling>\n",
    "\n",
    "<jouw resultatenbeschrijving>'\n",
    "\n",
    "\n",
    "Gemeente: {municipality}\n",
    "Groenpercentage: {greeneryPercentage}\n",
    "Groendoelstelling: {greeneryGoal}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"municipality\", \"greeneryPercentage\", \"greeneryGoal\"], template=template)\n",
    "\n",
    "analysis_chain = LLMChain(\n",
    "    llm=chosen_llm,\n",
    "    prompt=prompt_template,\n",
    "    output_key=\"greeneryAnalysis\"\n",
    ")\n",
    "\n",
    "print(\"Prompt die is meegegeven aan chain: \", prompt_template.format(municipality=\"[hier komt de gemeente te staan die is ingevoerd bij het starten van de sequential chain]\", greeneryPercentage=\"[hier komt het huidige groenpercentage te staan dat gevonden is in chain B]\", greeneryGoal=\"[hier komt de groendoelstelling te staan dat gevonden is in chain A]\"))"
   ],
   "id": "562dc10a3240d2a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt die is meegegeven aan chain:  \n",
      "Je bent een biodiversiteit analyst. Je krijgt een groenpercentage van een gemeente en de doelstelling van deze gemeente, aan jou de taak om deze informatie te tonen aan de gebruiker en te vertellen hoever de doelstelling behaald is. Jouw analyse komt als tekstje op een infographic te staan, beschrijf daarom de resultaten in maximaal 150 woorden. Zorg ervoor dat je een formele schrijfstijl gebruikt. Het format hiervoor is als volgt:\n",
      "'Gemeente: <de gemeente waar je het over hebt>\n",
      "Doelstelling: <het groenpercentage>\n",
      "Huidige hoeveelheid groen: <de groendoelstelling>\n",
      "\n",
      "<jouw resultatenbeschrijving>'\n",
      "\n",
      "\n",
      "Gemeente: [hier komt de gemeente te staan die is ingevoerd bij het starten van de sequential chain]\n",
      "Groenpercentage: [hier komt het huidige groenpercentage te staan dat gevonden is in chain B]\n",
      "Groendoelstelling: [hier komt de groendoelstelling te staan dat gevonden is in chain A]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 3: Combining the percentage and goal\n",
    "The third step is create a sequential chain that combines the Q&A chain and the SQL-agent chain. The sequential chain will parse the result of a previous chain to the next one with the use of prompt templates."
   ],
   "id": "d6f29c5ea9db144b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create sequential chain\n",
    "Create the sequential chain that combines the two chains created earlier."
   ],
   "id": "b4701e943a59617"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T19:08:00.143699Z",
     "start_time": "2025-05-19T19:07:40.800053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create combined chain\n",
    "greeneryChain = SequentialChain(\n",
    "    memory=SimpleMemory(),\n",
    "    chains=[greeneryPercentageChain, qa_chain, analysis_chain],\n",
    "    input_variables=[\"greeneryPercentageInputPrompt\", \"municipality\", \"query\"],\n",
    "    output_variables=[\"greeneryAnalysis\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "municipality = \"Schiedam\"\n",
    "\n",
    "# Execute combined chain\n",
    "result = greeneryChain.invoke({\"greeneryPercentageInputPrompt\": \"Vind het groenpercentage van gemeente \" + municipality, \"municipality\": municipality, \"query\": \"Wat is het percentage dat de gemeente wil realiseren met een natuurlijke opbouw?\"})\n",
    "print(result['greeneryAnalysis'])"
   ],
   "id": "fa54ddb7dc598703",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new SequentialChain chain...\u001B[0m\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Gemeente: Schiedam\n",
      "Doelstelling: 40%\n",
      "Huidige hoeveelheid groen: 46.52%\n",
      "\n",
      "De gemeente Schiedam toont een groenpercentage van 46.52%, wat de gestelde doelstelling van 40% overschrijdt. Hiermee ligt het huidige groenareaal 6.52 procentpunten boven de beoogde hoeveelheid. De gemeente heeft haar biodiversiteitsdoelstelling ruimschoots behaald, wat een positieve impact heeft op de lokale ecologie en leefomgeving. De doelstelling is daarmee voor 116.3% gerealiseerd.\n"
     ]
    }
   ],
   "execution_count": 68
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
