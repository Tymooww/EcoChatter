{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Biodiversity report generator\n",
    "---\n",
    "This notebook shows the inner workings of the report generator. It combines the \"greeneryDataForNeighborhoodV2\" and \"retrieveGreeneryGoals\" to create an overview of a greenery goal and the progress that has been made to achieve this goal.\n"
   ],
   "id": "e9eb7d5fa39038fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 0: Imports\n",
   "id": "308e71c660a7ab23"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:02:50.237427Z",
     "start_time": "2025-06-04T14:02:50.211970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains.sequential import SequentialChain\n",
    "from langchain.chains.retrieval_qa.base import RetrievalQA\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains import TransformChain\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.memory import SimpleMemory\n",
    "from langchain.agents import create_sql_agent\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "from langchain_community.docstore import InMemoryDocstore\n",
    "\n",
    "from google import genai\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import StaticPool\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from uuid import uuid4\n",
    "\n",
    "import tiktoken\n",
    "import requests\n",
    "import os\n",
    "import sqlite3\n",
    "import faiss"
   ],
   "id": "48fe2553ab822baf",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Choose a large language model and embedding model\n",
    "First the LLM and embedding model has to be chosen. There are many options, but a few have been tested and can easily be changed by runing it's specific codeblock."
   ],
   "id": "d2d8d2e222fb6696"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LLM\n",
    "Choose one of the implemented LLMs: gemma3 (local), qwen3 (local), o3-mini (cloud), gpt-4o-mini (cloud) or Gemini Flash 2.5 (cloud)\n"
   ],
   "id": "b05e975155a0d7e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option A: gemma3:4b (via Ollama, running locally)",
   "id": "44cced1d98b65da6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:21:44.597463Z",
     "start_time": "2025-06-04T14:21:44.564301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chosen_llm = ChatOllama(base_url='http://localhost:11434', model=\"gemma3:4b\")\n",
    "model_name = \"google/gemma-3-4b-it\"\n",
    "huggingFaceToken = os.environ.get(\"HF_token\")\n",
    "\n",
    "\n",
    "\n",
    "login(token = huggingFaceToken)"
   ],
   "id": "d89dc470d3c1b654",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65b032cd46d742bc92f6979abd8a5ab7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 141
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option B: qwen3:8b (via Ollama, running locally)",
   "id": "9cc163a04f5e827e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:13:18.479844Z",
     "start_time": "2025-06-04T14:13:18.457962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chosen_llm = ChatOllama(base_url='http://localhost:11434', model=\"qwen3:8b\")\n",
    "model_name = \"Qwen/Qwen3-8B\""
   ],
   "id": "b16cd8ce5d1e69bc",
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option C: o3-mini (via Microsoft Azure, running in cloud):",
   "id": "f77781c99b48898a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:23:16.499462Z",
     "start_time": "2025-06-04T14:23:16.436114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "chosen_llm = AzureChatOpenAI(model =\"o3-mini\", api_version=\"2025-01-01-preview\",azure_endpoint=\"https://56948-m9bdjgpg-eastus2.cognitiveservices.azure.com/openai/deployments/o3-mini/chat/completions?api-version=2025-01-01-preview\", api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"))\n",
    "model_name = \"o3-mini\""
   ],
   "id": "dae3e6325e0ba1a2",
   "outputs": [],
   "execution_count": 147
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option D: gpt-4o-mini (via Microsoft Azure, running in cloud):",
   "id": "5a9dce9213b110dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:28:51.859423Z",
     "start_time": "2025-06-04T14:28:51.827750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "chosen_llm = AzureChatOpenAI(model=\"gpt-4o-mini\", api_version=\"2025-01-01-preview\",\n",
    "                             azure_endpoint=\"https://56948-m9bdjgpg-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2025-01-01-preview\",\n",
    "                             api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"))\n",
    "model_name = \"gpt-4o-mini\""
   ],
   "id": "260f70cc78667853",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option E: Gemini 2.5 Flash (via Google AI platform, running in cloud):",
   "id": "a360451a477dcb76"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:26:23.790737Z",
     "start_time": "2025-06-04T14:26:23.726944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "chosen_llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-preview-04-17\",\n",
    "                                    api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "model_name = \"gemini-2.5-flash-preview-04-17\""
   ],
   "id": "87461f5cd0cb0c2a",
   "outputs": [],
   "execution_count": 161
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Embedding model\n",
    "Choose one of the implemented models: all-MiniLM-L6-v2 or all-mpnet-base-v2"
   ],
   "id": "c33ab39567730e0a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option A: all-MiniLM-L6-v2 (faster, but worse)",
   "id": "87baa1708087f6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:44:27.448825Z",
     "start_time": "2025-06-04T12:44:25.102754Z"
    }
   },
   "cell_type": "code",
   "source": "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")",
   "id": "70c92f2cf6c4481f",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Option B: all-mpnet-base-v2 (slower, but better)",
   "id": "4175768dc151a01e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:44:29.276406Z",
     "start_time": "2025-06-04T12:44:27.470351Z"
    }
   },
   "cell_type": "code",
   "source": "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")",
   "id": "4db0e52535159ea5",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 2: initialize token counters\n",
    "In this step the token counters for all chains will be initialized. Every model has a different tokenizer, so it is important to select the correct one for each model."
   ],
   "id": "3e0201d0b07687b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:28:54.614322Z",
     "start_time": "2025-06-04T14:28:54.603306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Choose corresponding tokenizer\n",
    "if model_name==\"Qwen/Qwen3-8B\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer_func = tokenizer.encode\n",
    "elif model_name==\"google/gemma-3-4b-it\":\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=huggingFaceToken)\n",
    "    tokenizer_func = tokenizer.encode\n",
    "elif model_name==\"o3-mini\" or model_name==\"gpt-4o-mini\":\n",
    "    tokenizer_func = tiktoken.encoding_for_model(model_name).encode\n",
    "elif model_name==\"gemini-2.5-flash-preview-04-17\":\n",
    "    tokenizer_func = genai.Client().models.count_tokens\n",
    "\n",
    "# Token counter greenery goal chain\n",
    "def greeneryGoalTokenCounter(tokenizer_function, prompt):\n",
    "    countedTokens = 0\n",
    "    # Count the tokens for the prompt(s)\n",
    "    if model_name==\"gemini-2.5-flash-preview-04-17\":\n",
    "        tokens = tokenizer_function(model=model_name, contents=prompt)\n",
    "        countedTokens += tokens.total_tokens\n",
    "    else:\n",
    "        countedTokens += len(tokenizer_func(prompt))\n",
    "\n",
    "    return countedTokens\n",
    "\n",
    "# Token counter callback greenery percentage chain\n",
    "class TokenCountingHandler(BaseCallbackHandler):\n",
    "    def __init__(self, tokenizer_function):\n",
    "        self.tokenizer_function = tokenizer_function\n",
    "        self.countedTokens = 0\n",
    "\n",
    "    def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "        # Count the tokens for the prompt(s)\n",
    "        for prompt in prompts:\n",
    "            if model_name==\"gemini-2.5-flash-preview-04-17\":\n",
    "                tokens = self.tokenizer_function(model=model_name, contents=prompt)\n",
    "                self.countedTokens += tokens.total_tokens\n",
    "            else:\n",
    "                self.countedTokens += len(self.tokenizer_function(prompt))\n",
    "\n",
    "    def on_llm_end(self, response, **kwargs):\n",
    "        # Count the tokens for the respons(es)\n",
    "        if 'generations' in response:\n",
    "            for generation in response['generations']:\n",
    "                text = generation[0].text\n",
    "                self.countedTokens += len(self.tokenizer_function(text))"
   ],
   "id": "82dd912b84e02c77",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 3: Create the chains\n",
    "The second step is to create the chains that will retrieve the information that is needed to generate biodiversity reports. This information consists of a greenery goal and a greenery percentage of a specific municipality."
   ],
   "id": "55d98a768d01d87b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chain A: Retrieve greenery goal\n",
    "The first chain is able to retrieve a greenery goal. This goal will be retrieved with a Q&A chain that uses RAG to analyse multiple document chunks. The data that the chain needs will first be prepared to help the LLM analyse the information better and after that the chain will be created that will (later) analyse the prepared data."
   ],
   "id": "fd68ef7eaca25039"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data preparation: save document chunks in a vector database\n",
    "First a pdf that contains a greenery goal will be loaded. The text of this document will be extracted and will then be chunked into smaller pieces. After that a vector database will be created and the chunks will be saved into it."
   ],
   "id": "e3931f1463f62da3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T12:44:31.800611Z",
     "start_time": "2025-06-04T12:44:29.324124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load file\n",
    "file_path = \"./GroenvisieSchiedam.pdf\"\n",
    "doc_reader = PdfReader(file_path)\n",
    "\n",
    "# Extract page content\n",
    "raw_text = ''\n",
    "for page in doc_reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text\n",
    "\n",
    "print(\"Amount of characters raw text: \" + str(len(raw_text)))\n",
    "\n",
    "# Split the page content into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    ")\n",
    "chunks = text_splitter.split_text(raw_text)\n",
    "\n",
    "print(\"Amount of characters first chunk: \" + str(len(chunks[0])))\n",
    "\n",
    "# Create vector store\n",
    "index = faiss.IndexFlatL2(len(embedding_model.embed_query(chunks[0]))) # Calculates the amount of dimensions the chunk's vector has\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embedding_model,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "# Give chunks UUIDS\n",
    "uuids = []\n",
    "for chunk in range(len(chunks)):\n",
    "    uuids.append(str(uuid4()))\n",
    "\n",
    "# Create embeddings of chunks and store them in the vector database\n",
    "vector_store.add_texts(texts=chunks, ids=uuids)\n",
    "\n",
    "print(\"\\n UUIDs of items stored in vector database: \" + str(uuids))"
   ],
   "id": "19a6ce21b1b45da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of characters raw text: 9039\n",
      "Amount of characters first chunk: 627\n",
      "\n",
      " UUIDs of items stored in vector database: ['8625428e-9ac8-4bba-82a2-1895ecd0d9ec', 'ee29895c-89ea-42a5-a668-10b6d889ccc0', '180fef72-7f39-4684-a3b9-d1d2b5162830', '12cc7d36-ddb6-4959-acb8-12f73a14901c', '09c80bba-bb38-49a7-a973-217d4d80555d', 'c077625d-297c-462e-baf8-3e5864157bf1', '2881772f-415c-4f50-b703-8cfc0812ba6c', '70d566ff-32f1-4ad6-a3e5-e9717d71794b', 'f5c2ff4f-7e37-4970-a823-c2f30e710452', '2c52cd16-110e-4fa1-bf7c-c7b463b5af69', 'd7236400-2caf-44ef-93b3-24c5bd1c24a3', '37ee0f05-28dc-4e06-9373-c5a68081c9b8']\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Q&A chain\n",
    "Create a QA chain that is able to do a similarity search for the 10 most relevant chunks and analyze these chunks to find the percentage asked."
   ],
   "id": "7b12f0edf62ef178"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:28:59.315450Z",
     "start_time": "2025-06-04T14:28:59.289220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create prompt template\n",
    "template = \"\"\"\n",
    "Je bent een documentenanalist. Gebruik de aangeleverde documenten om het percentage te vinden dat de vraag beantwoord. Geef als antwoord alleen het percentage, verder niks.\n",
    "\n",
    "Vraag: {question}\n",
    "Aangeleverde documenten: {context}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=['context', 'question'], template=template)\n",
    "\n",
    "# Create Q&A chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chosen_llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 10}), # This will do the similarity search for the 10 most relevant chunks and adds them to the {context} variable\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template},\n",
    "    output_key=\"greeneryGoal\"\n",
    ")\n",
    "\n",
    "# Transform function that returns the result of the agent in a dictionary (making it compatible with the other chains)\n",
    "def run_qa_chain(inputs):\n",
    "    # Calculate the tokens of the input\n",
    "    countedTokensInput = greeneryGoalTokenCounter(tokenizer_func, inputs[\"query\"])\n",
    "\n",
    "    # Run the chain\n",
    "    result = qa_chain.invoke(inputs[\"query\"])\n",
    "\n",
    "    # Put all document chunks in one string\n",
    "    document_contents = \"\"\n",
    "    for document in result[\"source_documents\"]:\n",
    "        document_contents += document.page_content\n",
    "\n",
    "    # Calculate the tokens of the documents and output\n",
    "    countedTokensDocuments = greeneryGoalTokenCounter(tokenizer_func, document_contents)\n",
    "    countedTokensOutput = greeneryGoalTokenCounter(tokenizer_func, result['greeneryGoal'])\n",
    "\n",
    "    # Calculate the amount of tokens used\n",
    "    totalTokens = countedTokensInput + countedTokensDocuments + countedTokensOutput\n",
    "    print(f\"Tokens used for retrieving greenery goal: {totalTokens}\")\n",
    "\n",
    "    return {\n",
    "        \"greeneryGoal\": result['greeneryGoal'],\n",
    "        \"tokenCount\": inputs[\"tokenCount\"] + totalTokens\n",
    "    }\n",
    "\n",
    "# Transform Chain that calls the transform function\n",
    "greeneryGoalChain = TransformChain(\n",
    "    input_variables=[\"query\", \"tokenCount\"],\n",
    "    output_variables=[\"greeneryGoal\"],\n",
    "    transform=run_qa_chain\n",
    ")"
   ],
   "id": "46f86546e27d844c",
   "outputs": [],
   "execution_count": 169
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chain B: Retrieve greenery percentage\n",
    "The second chain is able retreive the greenery percentage of a specific municipality. This is done with a SQL-agent chain. The data that the agent needs will first be prepared to help the agent analyse the information better and after that the chain will be created that will (later) analyse the prepared data."
   ],
   "id": "5427c2a36e9cedf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data preparation: store greenery percentages in SQlite database\n",
    "First the database will be created by building a SQlite database and creating a \"database engine\" object that will be used for creating the agent's toolkit. As soon as that is done the dataset will be retrieved via an API-request and will then be stored in the database."
   ],
   "id": "742a42304e704597"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:23:28.841768Z",
     "start_time": "2025-06-04T14:23:28.609025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create database\n",
    "con = sqlite3.connect(\"greeneryPercentages.db\", check_same_thread=False)\n",
    "cur = con.cursor()\n",
    "\n",
    "# Create municipalities table in database\n",
    "try:\n",
    "    cur.execute(\"CREATE TABLE municipalities(name varchar(255), greeneryPercentage float)\")\n",
    "    print(\"Table created successfully.\")\n",
    "\n",
    "except:\n",
    "    cur.execute(\"DROP TABLE municipalities\")\n",
    "    cur.execute(\"CREATE TABLE municipalities(name varchar(255), greeneryPercentage float)\")\n",
    "    print(\"Table has been reset, because it already existed.\")\n",
    "\n",
    "# Create database engine object\n",
    "engine = create_engine(\n",
    "        \"sqlite://\",\n",
    "        creator=lambda: con,\n",
    "        poolclass=StaticPool,\n",
    "        connect_args={\"check_same_thread\": False},\n",
    "    )\n",
    "db = SQLDatabase(engine)\n",
    "\n",
    "# Retrieve dataset\n",
    "dataset = requests.get('https://data.rivm.nl/geo/ank/ows?service=WFS&request=GetFeature&typeName=rivm_2022_groenpercentage_kaart_per_gemeente&propertyName=gm_naam,_mean&outputFormat=json').json()\n",
    "\n",
    "# Extract properties and put them into a list\n",
    "properties = []\n",
    "for feature in dataset[\"features\"]:\n",
    "    property = feature[\"properties\"]\n",
    "    properties.append((property[\"gm_naam\"], property[\"_mean\"]))\n",
    "\n",
    "# Save the items stored in the list into the database\n",
    "i=0\n",
    "while i < len(properties):\n",
    "    cur.execute(\"INSERT INTO municipalities VALUES (?,?)\", properties[i])\n",
    "    i = i + 1\n",
    "con.commit()"
   ],
   "id": "9b073020630bf673",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table has been reset, because it already existed.\n"
     ]
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create SQL-agent chain\n",
    "Create a SQL-agent chain that is able to analyse the created database and create a SQL-query for it to answer the question when it is executed."
   ],
   "id": "f4fc9eec4fd9d2bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:29:02.526780Z",
     "start_time": "2025-06-04T14:29:02.502381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create toolkit:\n",
    "toolkit = SQLDatabaseToolkit(db=db,llm=chosen_llm)\n",
    "\n",
    "# Create agent chain\n",
    "sql_agent_chain = create_sql_agent(\n",
    "    llm=chosen_llm,\n",
    "    toolkit=toolkit,\n",
    "    verbose=False,\n",
    "    output_key=\"greeneryPercentage\",\n",
    "    handleParsingErrors=True\n",
    ")\n",
    "\n",
    "# Transform function that returns the result and used tokens in a dictionary (making it compatible with the other chains)\n",
    "def run_sql_agent(inputs):\n",
    "    handler = TokenCountingHandler(tokenizer_func)\n",
    "\n",
    "    result = sql_agent_chain.run(\n",
    "        inputs[\"greeneryPercentageInputPrompt\"],\n",
    "        callbacks=[handler]\n",
    "    )\n",
    "\n",
    "    print(f\"Tokens used for retrieving greenery percentage: {handler.countedTokens}\")\n",
    "    return {\n",
    "        \"greeneryPercentage\": result,\n",
    "        \"tokenCount\": inputs[\"tokenCount\"] + handler.countedTokens\n",
    "    }\n",
    "\n",
    "# Transform Chain that calls the transform function\n",
    "greeneryPercentageChain = TransformChain(\n",
    "    input_variables=[\"greeneryPercentageInputPrompt\"],\n",
    "    output_variables=[\"greeneryPercentage\"],\n",
    "    transform=run_sql_agent\n",
    ")"
   ],
   "id": "15fa21b9639e94f4",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Chain C: Combine greenery goal and greenery percentage\n",
    "The third chain will analyse a greenery goal and percentage and write a few lines about the progress towards the goal.\n"
   ],
   "id": "a56af0b925d1e5bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:29:04.994296Z",
     "start_time": "2025-06-04T14:29:04.978450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create prompt template\n",
    "template = \"\"\"\n",
    "Je bent een biodiversiteit analyst. Je krijgt een groenpercentage van een gemeente en de doelstelling van deze gemeente, aan jou de taak om deze informatie te tonen aan de gebruiker en te vertellen hoever de doelstelling behaald is. Jouw analyse komt als tekstje op een infographic te staan, beschrijf daarom de resultaten in maximaal 150 woorden. Zorg ervoor dat je een formele schrijfstijl gebruikt. Het format hiervoor is als volgt:\n",
    "'Gemeente: <de gemeente waar je het over hebt>\n",
    "Doelstelling: <het groenpercentage>\n",
    "Huidige hoeveelheid groen: <de groendoelstelling>\n",
    "\n",
    "<jouw resultatenbeschrijving>\n",
    "\n",
    "\n",
    "\n",
    "Gemeente: {municipality}\n",
    "Groenpercentage: {greeneryPercentage}\n",
    "Groendoelstelling: {greeneryGoal}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"municipality\", \"greeneryPercentage\", \"greeneryGoal\"], template=template)\n",
    "\n",
    "analysis_chain = LLMChain(\n",
    "    llm=chosen_llm,\n",
    "    prompt=prompt_template,\n",
    "    output_key=\"greeneryAnalysis\"\n",
    ")\n",
    "\n",
    "# Transform function that returns the result of the agent in a dictionary (making it compatible with the other chains)\n",
    "def run_analysis_chain(inputs):\n",
    "    # Calculate the tokens of the input\n",
    "    countedTokensInput = greeneryGoalTokenCounter(tokenizer_func, prompt_template.format(municipality=inputs[\"municipality\"], greeneryPercentage=inputs[\"greeneryPercentage\"], greeneryGoal=inputs[\"greeneryGoal\"]))\n",
    "\n",
    "    # Run the chain\n",
    "    result = analysis_chain.invoke({\n",
    "    \"municipality\": inputs[\"municipality\"],\n",
    "    \"greeneryPercentage\": inputs[\"greeneryPercentage\"],\n",
    "    \"greeneryGoal\": inputs[\"greeneryGoal\"]\n",
    "})\n",
    "\n",
    "    # Calculate the tokens of the output\n",
    "    countedTokensOutput = greeneryGoalTokenCounter(tokenizer_func, result['greeneryAnalysis'])\n",
    "\n",
    "    # Calculate the amount of tokens used\n",
    "    totalTokensAnalysis = countedTokensInput + countedTokensOutput\n",
    "    print(f\"Tokens used for generating analysis: {totalTokensAnalysis}\")\n",
    "    total_tokens = inputs[\"tokenCount\"] + totalTokensAnalysis\n",
    "    print(f\"Total amount of tokens used: {total_tokens}\")\n",
    "    return {\n",
    "        \"greeneryAnalysis\": result['greeneryAnalysis'],\n",
    "        \"tokenCount\": inputs[\"tokenCount\"] + totalTokensAnalysis\n",
    "    }\n",
    "\n",
    "# Transform Chain that calls the transform function\n",
    "analysisChain = TransformChain(\n",
    "    input_variables=[\"municipality\", \"greeneryPercentage\", \"greeneryGoal\", \"tokenCount\", \"query\"],\n",
    "    output_variables=[\"greeneryAnalysis\"],\n",
    "    transform=run_analysis_chain\n",
    ")"
   ],
   "id": "562dc10a3240d2a7",
   "outputs": [],
   "execution_count": 172
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 4: Combining the percentage and goal\n",
    "The third step is create a sequential chain that combines the Q&A chain and the SQL-agent chain. The sequential chain will parse the result of a previous chain to the next one with the use of prompt templates."
   ],
   "id": "d6f29c5ea9db144b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create sequential chain\n",
    "Create the sequential chain that combines the two chains created earlier."
   ],
   "id": "b4701e943a59617"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-04T14:29:43.872342Z",
     "start_time": "2025-06-04T14:29:32.108430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create combined chain\n",
    "greeneryChain = SequentialChain(\n",
    "    memory=SimpleMemory(),\n",
    "    chains=[greeneryPercentageChain, greeneryGoalChain, analysisChain],\n",
    "    input_variables=[\"greeneryPercentageInputPrompt\", \"municipality\", \"query\", \"tokenCount\"],\n",
    "    output_variables=[\"greeneryAnalysis\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "municipality = \"Schiedam\"\n",
    "\n",
    "# Execute combined chain\n",
    "result = greeneryChain.invoke({\"greeneryPercentageInputPrompt\": \"Vind het groenpercentage van gemeente \" + municipality, \"municipality\": municipality, \"query\": \"Wat is het percentage dat de gemeente wil realiseren met een natuurlijke opbouw?\", \"tokenCount\":0})\n",
    "print(result[\"greeneryAnalysis\"])"
   ],
   "id": "fa54ddb7dc598703",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001B[1m> Entering new SequentialChain chain...\u001B[0m\n",
      "Tokens used for retrieving greenery percentage: 2604\n",
      "Tokens used for retrieving greenery goal: 2026\n",
      "Tokens used for generating analysis: 346\n",
      "Total amount of tokens used: 4976\n",
      "\n",
      "\u001B[1m> Finished chain.\u001B[0m\n",
      "Gemeente: Schiedam  \n",
      "Doelstelling: 40%  \n",
      "Huidige hoeveelheid groen: 46.52%  \n",
      "\n",
      "De gemeente Schiedam heeft zijn groenpercentage vastgesteld op 46.52%, wat aanzienlijk boven de gestelde doelstelling van 40% ligt. Dit resultaat toont aan dat Schiedam met succes heeft bijgedragen aan het behoud en de uitbreiding van zijn groene ruimte. De huidige groene ruimte draagt niet alleen bij aan de biodiversiteit, maar verhoogt ook de kwaliteit van de leefomgeving voor de inwoners. De gemeente kan zich nu richten op de verdere verbetering van de kwaliteit van het groen en het stimuleren van biodiversiteitsprojecten, om zo de ecologische duurzaamheid te waarborgen en een gezondere leefomgeving te creÃ«ren. De overschrijding van de doelstelling weerspiegelt de inzet van de gemeente in het bevorderen van een groene en gezonde stad.\n"
     ]
    }
   ],
   "execution_count": 174
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
